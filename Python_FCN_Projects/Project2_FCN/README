CS 4700 / CS 5700 - Network Fundamentals: Project2

Approach:
1) The project requires to crawl through all the web pages of the Fakebook website to find all the secret flags for specific set of login credentials.
2) The client is implemented using Python language and the program executed by calling a bash script which accepts credentials from the command line and passes it to the python program.
3) The client initially requests for the login page of Fakebook from the server and then posts the credentials to the login page along with CSRF verification token id and Session id. 	Once the login is successful then the client requests for the redirected URL in the "Location:" header along with the new session id.
4) The client collects all the the URL's of the Fakebook website in the webpage by parsing it using a subclass of the HTMLParser module(by finding the anchor tags).  
5) The collected URL's are appended to the frontier list and the client crawls through the URL's in the frontier after checking if the URL has not been traversed earlier and the process continues by parsing the webpages and appending the URL's to the frontier.
6) Status codes have been handled according to the requirements.
7) All the exceptions have been handled including the case of invalid credentials. The program terminates once the 5 secret flags are found.

Challenges faced:
1) The client was abruptly terminating when the server returned the "500 - Internal Server Error". It was diffcult to figure out the reason as to why the server was not sending any response for certain amount of time. The challenge was overcome when the Connection header in the response was observed and we found out that the socket connection was getting closed.
Thus we created a new socket and re-connected to the server and sent the GET request until the requested html page is received.
2) A similar challenge was faced when the server was returning a "0" response. To handle this we were trying to check if the length of the response from the server was 1. But, the server was actually responding  the "0" response with a length of 5. Once we resolved this isssue we re-sent the GET message for the same URL and the continuity of the crawling was maintained.

Test Cases:

1) ./webcrawler 001280199 9S1PD388
Output: 
c436423e2262fa80fa6a33546413843911b4dcc6c9ce00de01ccf6d806b8bac0
2fe47a9216ca283734f692299cf210b16e0efd01cdbb2cb5efc5e687dfbd4d14
74037c4bd5357899d3141010c02d742378528ba98cb0797253d42e4ef95be887
4112acc7e7512420ca3977562b7b4beb286f3995d2702bdeac650ad7ac29e88f
6bb5319cd54d85305a7e7cc9d0d670a90817e4e714cd4eb9023340cbdba486e9

2) ./webcrawler 001256309 STEZZN5Y
Output: 
b3e058c619e8bd4777d1b6470e5873bc99c2e71971af30c48c972cd6f39f73a7
705ad2bf85da6faa69bc1ac9b8eafacee6b5947edf2e18dba23d2701b86e0373
f84088404cc39059530b4698d2513760873253e0393ac6fe8728336f4b3fe103
0349e565e86f8129bcdb112e5206e0e41363bdf6e35493c4fa8f488f74811704
f2ea59b37c2444a8bf249846e8624c229bafb3801d81cd71535fc9a99a140fc3

3) ./webcrawler 001280199 STEZZN5Y
Output: Invalid credentials

4) ./webcrawler 0012801 STEZZN
Output: Invalid credentials

5)./webcrawler 001280199876 STEZZN5Ytttt
Output: Invalid credentials
